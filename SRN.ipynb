{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import cycle\n",
    "from random import uniform\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SequenceGenerator:\n",
    "    def __init__(self, x, sequence_size):\n",
    "        if x.shape[0] < sequence_size:\n",
    "            raise ValueError(\"sequence_size can't be bigger than total number of samples\")\n",
    "            \n",
    "        self.sequence_size = sequence_size\n",
    "        self.x = x\n",
    "        self.samples_generated = 0\n",
    "        self.epochs = 0\n",
    "        self.samples_gen = self._samples_gen()\n",
    "            \n",
    "    def _samples_gen(self):        \n",
    "        for i in cycle(range(self.x.shape[0] - self.sequence_size - 1)):\n",
    "            sample_indices = range(i, i + self.sequence_size)\n",
    "            targets_indices = range(i + 1, i + self.sequence_size + 1)\n",
    "            batch = np.take(self.x, sample_indices).reshape(-1, 1)\n",
    "            target = np.take(self.x, targets_indices).reshape(-1, 1)\n",
    "\n",
    "            assert batch.shape == (self.sequence_size, 1)\n",
    "            assert target.shape == (self.sequence_size, 1)\n",
    "\n",
    "            yield batch, target\n",
    "        \n",
    "    def next_batch(self):\n",
    "        batch, target = next(self.samples_gen)\n",
    "        self.samples_generated += batch.shape[0]\n",
    "        if self.samples_generated >= self.x.shape[0]:\n",
    "            self.epochs += 1\n",
    "            self.samples_generated = self.x.shape[0] - self.samples_generated\n",
    "        return batch, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    def do_init(self, tensor_shape):\n",
    "        in_num, out_num = tensor_shape\n",
    "        variance = np.sqrt(2.0 / (in_num + out_num))\n",
    "        return np.random.normal(0, variance, size=tensor_shape)\n",
    "    \n",
    "class GaussianInitializer:\n",
    "    def do_init(self, tensor_shape, m=0, sd=0.01):\n",
    "        return np.random.normal(m, sd, tensor_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def update(self, lr, w, delta):\n",
    "        w += -lr * delta / self.batch_size\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SerializedWeights:\n",
    "    def __init__(self, rnn):\n",
    "        if isinstance(rnn, RNN) == False:\n",
    "            raise ValueError(\"rnn should be an instance of class RNN\")\n",
    "        self.sequence_size = rnn.sequence_size\n",
    "        self._shift = {}\n",
    "        self.units = rnn.units\n",
    "        self.w = self._w(rnn)\n",
    "    \n",
    "    def _w(self, rnn):\n",
    "        weights = []\n",
    "        weights.extend(rnn.w_in.flatten())\n",
    "        self._shift[\"w_in\"] = ((0, len(weights)), rnn.w_in.shape)\n",
    "        start = len(weights)\n",
    "        \n",
    "        weights.extend(rnn.w_rec.flatten())\n",
    "        self._shift[\"w_rec\"] = ((start, len(weights)), rnn.w_rec.shape)\n",
    "        start = len(weights)\n",
    "        \n",
    "        weights.extend(rnn.w_out.flatten())\n",
    "        self._shift[\"w_out\"] = ((start, len(weights)), rnn.w_out.shape)\n",
    "        start = len(weights)\n",
    "        \n",
    "        weights.extend(rnn.b_1.flatten())\n",
    "        self._shift[\"b_1\"] = ((start, len(weights)), rnn.b_1.shape)\n",
    "        start = len(weights)\n",
    "        \n",
    "        weights.extend(rnn.b_2.flatten())\n",
    "        self._shift[\"b_2\"] = ((start, len(weights)), rnn.b_2.shape)  \n",
    "        \n",
    "        return weights\n",
    "        \n",
    "    def save_grads(self, dw_in, dw_rec, dw_out, db_1, db_2):\n",
    "        grads = []\n",
    "        grads.extend(dw_in.flatten())\n",
    "        grads.extend(dw_rec.flatten())\n",
    "        grads.extend(dw_out.flatten())\n",
    "        grads.extend(db_1.flatten())\n",
    "        grads.extend(db_2.flatten())\n",
    "        self.g = grads\n",
    "    \n",
    "    def unpack(self):\n",
    "        weights = []\n",
    "        for idx, shape in self._shift.values():\n",
    "            wts = np.take(self.w, range(idx[0], idx[1])).reshape(shape)\n",
    "            weights.append(wts)\n",
    "            \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, units, sequence_size):\n",
    "        self.units = units\n",
    "        self.sequence_size = sequence_size\n",
    "    \n",
    "    #\n",
    "    # Initialize weights\n",
    "    #\n",
    "    def _initialize_tensors(self):\n",
    "        w_rec_shape = (self.units, self.units)\n",
    "        w_in_shape = (self.units, 1)\n",
    "        w_out_shape = (1, self.units)\n",
    "        b_1_shape = (self.units, 1)\n",
    "        b_2_shape = (1, 1)\n",
    "        \n",
    "        # Weights\n",
    "        self.w_rec = np.identity(self.units)  # Fashion, fashion, fashion...\n",
    "        self.w_in = self.initializer.do_init(w_in_shape)\n",
    "        self.w_out = self.initializer.do_init(w_out_shape)\n",
    "        self.b_1 = np.zeros(b_1_shape)\n",
    "        self.b_2 = np.zeros(b_2_shape)\n",
    "    \n",
    "    #\n",
    "    # Matrix to vector serialization \n",
    "    #\n",
    "    def srn2vec(self, dw_in, dw_rec, dw_out, db_1, db_2):\n",
    "        net = SerializedWeights(self)\n",
    "        net.save_grads(dw_in, dw_rec, dw_out, db_1, db_2)\n",
    "        return net\n",
    "    \n",
    "    #\n",
    "    # Vector to matrix deserialization\n",
    "    #\n",
    "    def vec2srn(self, serialized_net):\n",
    "        if isinstance(serialized_net, SerializedWeights) == False:\n",
    "            raise ValueError(\"serialized_net should be an instance of class SerializedWeights\")\n",
    "            \n",
    "        w_in, w_rec, w_out, b_1, b_2 = serialized_net.unpack()\n",
    "        assert np.allclose(w_in, self.w_in)\n",
    "        assert np.allclose(w_rec, self.w_rec)\n",
    "        assert np.allclose(w_out, self.w_out)\n",
    "        assert np.allclose(b_1, self.b_1)\n",
    "        assert np.allclose(b_2, self.b_2)\n",
    "        \n",
    "        return w_in, w_rec, w_out, b_1, b_2\n",
    "    \n",
    "    #\n",
    "    # Forward and backward propagation\n",
    "    #\n",
    "    def forward_backward(self, batch, target, initial_z, grad_clipping):\n",
    "        in_state, z_state, y_hat_state, loss_state = {}, {}, {}, {}\n",
    "        z_state[-1] = np.copy(initial_z)  # Inital state for z-1\n",
    "        dz_next = np.zeros_like(initial_z)  # Inital state for dz+1\n",
    "        dw_in = np.zeros_like(self.w_in)\n",
    "        dw_rec = np.zeros_like(self.w_rec)\n",
    "        dw_out = np.zeros_like(self.w_out)\n",
    "        db_1 = np.zeros_like(self.b_1)\n",
    "        db_2 = np.zeros_like(self.b_2)\n",
    "        loss = 0\n",
    "\n",
    "        # Forward prop\n",
    "        for i in range(batch.shape[0]):\n",
    "            # Pass through the first function\n",
    "            in_state[i] = batch[i, :].reshape(1, -1)\n",
    "            z_state[i] = np.tanh(self.w_in @ in_state[i] + self.w_rec @ z_state[i-1] + self.b_1)\n",
    "            assert z_state[i].shape == (self.units, batch.shape[-1])\n",
    "\n",
    "            # Pass through the second function\n",
    "            y_hat_state[i] = np.tanh(self.w_out @ z_state[i] + self.b_2)\n",
    "            assert y_hat_state[i].shape == (1, batch.shape[-1])\n",
    "\n",
    "            # Pass through the loss function\n",
    "            loss_state[i] = 0.5 * (y_hat_state[i] - target[i])**2\n",
    "            assert loss_state[i].shape == (1, batch.shape[-1])\n",
    "\n",
    "            loss += np.mean(loss_state[i])\n",
    "\n",
    "        # Backward prop\n",
    "        for i in reversed(range(batch.shape[0])):\n",
    "            # Pass back through the loss function\n",
    "            dy_hat = y_hat_state[i] - target[i]\n",
    "            assert dy_hat.shape == (1, batch.shape[-1])\n",
    "\n",
    "            # Pass back through the second function\n",
    "            da_2 = (1 - y_hat_state[i] * y_hat_state[i]) * dy_hat\n",
    "            dw_out += da_2 @ z_state[i].T\n",
    "            db_2 += np.sum(da_2, axis=1).reshape(-1, 1)\n",
    "            assert dw_out.shape == self.w_out.shape\n",
    "            assert db_2.shape == self.b_2.shape\n",
    "\n",
    "            # Pass back through the first function\n",
    "            dz = self.w_out.T @ da_2 + dz_next\n",
    "            da_1 = (1 - z_state[i] * z_state[i]) * dz\n",
    "            dw_rec += da_1 @ z_state[i-1].T\n",
    "            dw_in += da_1 @ in_state[i].T\n",
    "            db_1 += np.sum(da_1, axis=1).reshape(-1, 1)\n",
    "            dz_next = self.w_rec.T @ da_1\n",
    "            assert dw_rec.shape == self.w_rec.shape\n",
    "            assert dw_in.shape == self.w_in.shape\n",
    "            assert db_1.shape == self.b_1.shape\n",
    "\n",
    "        # Gradient clipping\n",
    "        if grad_clipping:\n",
    "            for delta in (dw_in, dw_rec, dw_out, db_1, db_2):\n",
    "                np.clip(delta, -1, 1, out=delta)\n",
    "\n",
    "        return loss, dw_in, dw_rec, dw_out, db_1, db_2, z_state[batch.shape[0] - 1]\n",
    "    \n",
    "    #\n",
    "    # Fit model to the data\n",
    "    #\n",
    "    def fit(self, x, learning_rate, epochs=10, grad_clipping=False):\n",
    "        self.batch_generator = SequenceGenerator(x, self.sequence_size)\n",
    "        self.initializer = XavierInitializer()\n",
    "        # self.initializer = GaussianInitializer()\n",
    "        self.opt = SGD(self.sequence_size)\n",
    "        self._initialize_tensors()\n",
    "        z_init = np.zeros((self.units, 1))\n",
    "        current_epoch = 1\n",
    "        current_iter = 1\n",
    "        epoch_loss = []\n",
    "        total_loss = []\n",
    "        \n",
    "        # Start training \n",
    "        while self.batch_generator.epochs < epochs:\n",
    "            batch, target = self.batch_generator.next_batch()\n",
    "\n",
    "            # Run forward, backward and return loss\n",
    "            loss, dw_in, dw_rec, dw_out, db_1, db_2, z_init = self.forward_backward(\n",
    "                batch, \n",
    "                target, \n",
    "                z_init, \n",
    "                grad_clipping\n",
    "            )\n",
    "\n",
    "            net_state = self.srn2vec(dw_in, dw_rec, dw_out, db_1, db_2)\n",
    "#             w_in, w_rec, w_out, b_1, b_2 = self.vec2srn(net_state)\n",
    "            \n",
    "            # Learning rate decay schedule\n",
    "            lr = learning_rate / np.sqrt(current_iter + 1)\n",
    "            \n",
    "            # Weight update\n",
    "            params = zip((self.w_in, self.w_rec, self.w_out, self.b_1, self.b_2),\n",
    "                         (dw_in, dw_rec, dw_out, db_1, db_2))\n",
    "            \n",
    "            for wts, grads in params:\n",
    "                self.opt.update(lr, wts, grads)\n",
    "\n",
    "            # Report progress\n",
    "            epoch_loss.append(loss)\n",
    "            \n",
    "            if self.batch_generator.epochs >= current_epoch:\n",
    "                avg_loss = np.mean(epoch_loss)\n",
    "                print(\"Epoch {}, loss: {:.6f}, weights norm: {:.2f}, gradients norm {:.2f}\".\\\n",
    "                      format(current_epoch, \n",
    "                             avg_loss,\n",
    "                             np.linalg.norm(net_state.w),\n",
    "                             np.linalg.norm(net_state.g))\n",
    "                     )\n",
    "\n",
    "                z_init = np.zeros_like(z_init)\n",
    "                epoch_loss = []\n",
    "                total_loss.append(avg_loss)\n",
    "                current_epoch += 1\n",
    "            \n",
    "            current_iter += 1\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    #        \n",
    "    # Numerical vs Analytic gradient check\n",
    "    #\n",
    "    def gradient_check(self):\n",
    "        batch, target = SequenceGenerator(x, self.sequence_size).next_batch()\n",
    "        self.initializer = XavierInitializer()\n",
    "        self._initialize_tensors()\n",
    "        initial_z = np.zeros((self.units, 1))\n",
    "        \n",
    "        num_checks = 10\n",
    "        eps = 1e-5\n",
    "        \n",
    "        _, dw_in, dw_rec, dw_out, db_1, db_2, _ = self.forward_backward(\n",
    "            batch, \n",
    "            target, \n",
    "            initial_z, \n",
    "            grad_clipping=False\n",
    "        )\n",
    "        \n",
    "        params = zip(\n",
    "            (self.w_in, self.w_rec, self.w_out, self.b_1, self.b_2),\n",
    "            (dw_in, dw_rec, dw_out, db_1, db_2),\n",
    "            (\"dw_in\", \"dw_rec\", \"dw_out\", \"db_1\", \"db_2\")\n",
    "        )\n",
    "        \n",
    "        for wts, delta, name in params:\n",
    "            assert wts.shape == delta.shape\n",
    "            print(\"\\nComparing {} randomly taken values for {}:\".format(num_checks, name))\n",
    "            \n",
    "            for _ in range(num_checks):\n",
    "                rand_idx = int(uniform(0, wts.size))\n",
    "                w = wts.flat[rand_idx]\n",
    "                \n",
    "                wts.flat[rand_idx] = w + eps\n",
    "                f0, _, _, _, _, _, _ = self.forward_backward(\n",
    "                    batch, \n",
    "                    target, \n",
    "                    initial_z, \n",
    "                    grad_clipping=False\n",
    "                )\n",
    "                \n",
    "                wts.flat[rand_idx] = w - eps\n",
    "                f1, _, _, _, _, _, _ = self.forward_backward(\n",
    "                    batch, \n",
    "                    target, \n",
    "                    initial_z, \n",
    "                    grad_clipping=False\n",
    "                )\n",
    "                wts.flat[rand_idx] = w\n",
    "                \n",
    "                numerical = (f0 - f1) / (2 * eps)\n",
    "                analytic = delta.flat[rand_idx]\n",
    "                error = abs(analytic - numerical) / abs(numerical + analytic)\n",
    "                \n",
    "                print(\"Relative error: {:.1E}, numerical: {:.4f}, analytic: {:.4f}\".\n",
    "                    format(error, numerical, analytic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MackeyGlass.csv') as f:\n",
    "    x = np.array([float(n) for n in f.readline().split(';')]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical vs Analytic gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing 10 randomly taken values for dw_in:\n",
      "Relative error: 9.0E-11, numerical: 0.4602, analytic: 0.4602\n",
      "Relative error: 3.1E-11, numerical: 0.0261, analytic: 0.0261\n",
      "Relative error: 4.2E-11, numerical: -0.2354, analytic: -0.2354\n",
      "Relative error: 9.9E-12, numerical: -0.0605, analytic: -0.0605\n",
      "Relative error: 6.3E-11, numerical: 0.6280, analytic: 0.6280\n",
      "Relative error: 4.0E-11, numerical: -0.1812, analytic: -0.1812\n",
      "Relative error: 3.2E-11, numerical: -0.2817, analytic: -0.2817\n",
      "Relative error: 9.0E-11, numerical: 0.4602, analytic: 0.4602\n",
      "Relative error: 4.2E-11, numerical: -0.5609, analytic: -0.5609\n",
      "Relative error: 6.2E-11, numerical: 0.6280, analytic: 0.6280\n",
      "\n",
      "Comparing 10 randomly taken values for dw_rec:\n",
      "Relative error: 3.0E-11, numerical: -0.0085, analytic: -0.0085\n",
      "Relative error: 1.1E-11, numerical: -0.1807, analytic: -0.1807\n",
      "Relative error: 8.6E-12, numerical: -0.0577, analytic: -0.0577\n",
      "Relative error: 5.8E-11, numerical: -0.1358, analytic: -0.1358\n",
      "Relative error: 1.8E-11, numerical: -0.0853, analytic: -0.0853\n",
      "Relative error: 3.3E-11, numerical: -0.0835, analytic: -0.0835\n",
      "Relative error: 9.3E-11, numerical: 0.2531, analytic: 0.2531\n",
      "Relative error: 6.5E-11, numerical: -0.1634, analytic: -0.1634\n",
      "Relative error: 3.8E-11, numerical: 0.1444, analytic: 0.1444\n",
      "Relative error: 3.5E-11, numerical: 0.2839, analytic: 0.2839\n",
      "\n",
      "Comparing 10 randomly taken values for dw_out:\n",
      "Relative error: 3.1E-14, numerical: -0.3541, analytic: -0.3541\n",
      "Relative error: 1.2E-12, numerical: -0.4327, analytic: -0.4327\n",
      "Relative error: 1.2E-12, numerical: 0.2542, analytic: 0.2542\n",
      "Relative error: 1.2E-12, numerical: -0.4327, analytic: -0.4327\n",
      "Relative error: 2.6E-12, numerical: -0.1594, analytic: -0.1594\n",
      "Relative error: 2.0E-12, numerical: -0.3030, analytic: -0.3030\n",
      "Relative error: 1.8E-13, numerical: 0.2542, analytic: 0.2542\n",
      "Relative error: 1.5E-13, numerical: -0.0463, analytic: -0.0463\n",
      "Relative error: 1.1E-12, numerical: 0.2540, analytic: 0.2540\n",
      "Relative error: 1.0E-11, numerical: 0.0095, analytic: 0.0095\n",
      "\n",
      "Comparing 10 randomly taken values for db_1:\n",
      "Relative error: 3.2E-10, numerical: -0.3454, analytic: -0.3454\n",
      "Relative error: 4.8E-12, numerical: -0.1996, analytic: -0.1996\n",
      "Relative error: 5.4E-10, numerical: -0.7762, analytic: -0.7762\n",
      "Relative error: 1.0E-09, numerical: -2.2042, analytic: -2.2042\n",
      "Relative error: 5.5E-10, numerical: -0.5987, analytic: -0.5987\n",
      "Relative error: 6.6E-12, numerical: -0.1996, analytic: -0.1996\n",
      "Relative error: 3.2E-10, numerical: -0.3454, analytic: -0.3454\n",
      "Relative error: 5.3E-10, numerical: -0.6144, analytic: -0.6144\n",
      "Relative error: 7.2E-10, numerical: 1.0824, analytic: 1.0824\n",
      "Relative error: 5.4E-10, numerical: -0.7762, analytic: -0.7762\n",
      "\n",
      "Comparing 10 randomly taken values for db_2:\n",
      "Relative error: 6.2E-12, numerical: -1.0183, analytic: -1.0183\n",
      "Relative error: 6.6E-12, numerical: -1.0183, analytic: -1.0183\n",
      "Relative error: 6.2E-12, numerical: -1.0183, analytic: -1.0183\n",
      "Relative error: 6.6E-12, numerical: -1.0183, analytic: -1.0183\n",
      "Relative error: 6.6E-12, numerical: -1.0183, analytic: -1.0183\n",
      "Relative error: 6.2E-12, numerical: -1.0183, analytic: -1.0183\n",
      "Relative error: 6.6E-12, numerical: -1.0183, analytic: -1.0183\n",
      "Relative error: 6.2E-12, numerical: -1.0183, analytic: -1.0183\n",
      "Relative error: 6.6E-12, numerical: -1.0183, analytic: -1.0183\n",
      "Relative error: 6.2E-12, numerical: -1.0183, analytic: -1.0183\n"
     ]
    }
   ],
   "source": [
    "net = RNN(units=16, sequence_size=5)\n",
    "net.gradient_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.031879, weights norm: 11.48, gradients norm 1.00\n",
      "Epoch 2, loss: 0.020059, weights norm: 11.48, gradients norm 0.93\n",
      "Epoch 3, loss: 0.014394, weights norm: 11.48, gradients norm 0.67\n",
      "Epoch 4, loss: 0.011931, weights norm: 11.48, gradients norm 0.06\n",
      "Epoch 5, loss: 0.010387, weights norm: 11.48, gradients norm 0.83\n",
      "Epoch 6, loss: 0.009306, weights norm: 11.48, gradients norm 1.54\n",
      "Epoch 7, loss: 0.008489, weights norm: 11.48, gradients norm 1.66\n",
      "Epoch 8, loss: 0.007794, weights norm: 11.48, gradients norm 1.18\n",
      "Epoch 9, loss: 0.007172, weights norm: 11.48, gradients norm 0.60\n",
      "Epoch 10, loss: 0.006623, weights norm: 11.48, gradients norm 0.23\n",
      "Epoch 11, loss: 0.006150, weights norm: 11.48, gradients norm 0.00\n",
      "Epoch 12, loss: 0.005752, weights norm: 11.48, gradients norm 0.16\n",
      "Epoch 13, loss: 0.005425, weights norm: 11.48, gradients norm 0.24\n",
      "Epoch 14, loss: 0.005162, weights norm: 11.48, gradients norm 0.20\n",
      "Epoch 15, loss: 0.004959, weights norm: 11.48, gradients norm 0.03\n",
      "Epoch 16, loss: 0.004809, weights norm: 11.48, gradients norm 0.21\n",
      "Epoch 17, loss: 0.004708, weights norm: 11.48, gradients norm 0.43\n",
      "Epoch 18, loss: 0.004649, weights norm: 11.48, gradients norm 0.57\n",
      "Epoch 19, loss: 0.004624, weights norm: 11.48, gradients norm 0.61\n",
      "Epoch 20, loss: 0.004633, weights norm: 11.48, gradients norm 0.61\n",
      "Epoch 21, loss: 0.004677, weights norm: 11.48, gradients norm 0.57\n",
      "Epoch 22, loss: 0.004753, weights norm: 11.48, gradients norm 0.37\n",
      "Epoch 23, loss: 0.004877, weights norm: 11.48, gradients norm 0.05\n",
      "Epoch 24, loss: 0.005080, weights norm: 11.48, gradients norm 0.52\n",
      "Epoch 25, loss: 0.005287, weights norm: 11.48, gradients norm 0.91\n"
     ]
    }
   ],
   "source": [
    "net = RNN(units=128, sequence_size=1)\n",
    "progress = net.fit(x, learning_rate=5e-2, epochs = 25, grad_clipping=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnAAAAEqCAYAAACV7m2vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118b6fbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.plot(progress)\n",
    "fig.suptitle('Learning curve', fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('MSE', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
